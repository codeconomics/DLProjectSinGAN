{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imgaug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import inspect\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "torch.manual_seed(1)\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torch.utils.data import DataLoader \n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch_SinGAN import *\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "class TestResNet(ResNet):\n",
    "    def __init__(self, channels=3, num_classes=10):\n",
    "        super(TestResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "        self.conv1 = torch.nn.Conv2d(channels, 64, \n",
    "            kernel_size=(7, 7), \n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3), bias=False)\n",
    "\n",
    "def calculate_metric(metric_fn, true_y, pred_y):\n",
    "    # multi class problems need to have averaging method\n",
    "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
    "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
    "    else:\n",
    "        return metric_fn(true_y, pred_y)\n",
    "\n",
    "def make_data_loaders(X_train, y_train,batch_size=100):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state=42)\n",
    "    data_transform = Compose([Resize((224, 224))])\n",
    "    X_train = torch.from_numpy(X_train).type(torch.float)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "\n",
    "    X_val = torch.from_numpy(X_val).type(torch.float)\n",
    "    y_val = torch.from_numpy(y_val).type(torch.LongTensor)\n",
    "\n",
    "    train_d = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_d, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    val_d = torch.utils.data.TensorDataset(X_val,y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_d,batch_size = batch_size, shuffle = False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def make_test_loarder(X_test, y_test, batch_size=100):\n",
    "    X_test = torch.from_numpy(X_test).type(torch.float)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "    test_d = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_d, batch_size = batch_size, shuffle = False)\n",
    "    return test_loader\n",
    "\n",
    "def print_scores(p, r, f1, a, batch_size):\n",
    "    # just an utility printing function\n",
    "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
    "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")\n",
    "\n",
    "def compute_test(model, X_test, y_test, c, batch_size=100):\n",
    "    X_test = X_test.transpose(0,3,1,2)\n",
    "    test_loader = make_test_loarder(X_test, y_test, batch_size=batch_size)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = criterion.cuda()\n",
    "    class_correct = 0\n",
    "    class_total = 0\n",
    "    model.eval()\n",
    "    test_losses = 0\n",
    "    precision, recall, f1, accuracy = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            outputs = model(X) # this get's the prediction from the network\n",
    "\n",
    "            test_losses += criterion(outputs, y)\n",
    "\n",
    "            predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
    "            \n",
    "            # calculate P/R/F1/A metrics for batch\n",
    "            for acc, metric in zip((precision, recall, f1, accuracy), \n",
    "                                    (precision_score, recall_score, f1_score, accuracy_score)):\n",
    "                acc.append(\n",
    "                    calculate_metric(metric, y.cpu(), predicted_classes.cpu())\n",
    "                )\n",
    "            class_correct += np.sum((predicted_classes.cpu().data.numpy().astype(int) == c) & (y.cpu().data.numpy().astype(int) == c))\n",
    "            class_total += np.sum(y.cpu().data.numpy().astype(int) == c)\n",
    "            \n",
    "            \n",
    "    print(f\"test loss: {test_losses/len(test_loader)}\")\n",
    "    print_scores(precision, recall, f1, accuracy, batch_size)\n",
    "    print(f\"accuracy for class {c}: {class_correct/class_total}\")\n",
    "    print(class_correct, class_total)\n",
    "    \n",
    "    \n",
    "def av_SSIM(images, other=None, pairs=1000):\n",
    "    l = np.zeros(pairs)\n",
    "    if other:\n",
    "        ind_a = np.random.choice(list(range(images.shape[0])), size = pairs)\n",
    "        ind_b = np.random.choice(list(range(other.shape[0])), size = pairs)\n",
    "    else:\n",
    "        ind_a = np.random.choice(list(range(images.shape[0])), size = pairs)\n",
    "        ind_b = np.zeros(pairs, dtype=int)\n",
    "        count = 0\n",
    "        while count < pairs:\n",
    "            ind_b[count] = np.random.choice(list(range(images.shape[0])), size = 1)[0]\n",
    "            if ind_a[count] != ind_b[count]:\n",
    "                count += 1\n",
    "    \n",
    "    for i in range(pairs):\n",
    "        if other:\n",
    "            l[i] = ssim(images[ind_a[i]], other[ind_b[i]], data_range=1, multichannel=True)\n",
    "        else:\n",
    "            l[i] = ssim(images[ind_a[i]], images[ind_b[i]], data_range=1, multichannel=True)\n",
    "    \n",
    "    return l.mean()\n",
    " \n",
    "    \n",
    "def get_classic_aug():\n",
    "    ia.seed(42)\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Crop(percent=(0, 0.1)),\n",
    "        iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "            rotate=(-25, 25),\n",
    "            shear=(-8, 8)\n",
    "        )\n",
    "    ], random_order=True) \n",
    "    return seq\n",
    "    \n",
    "def train(model, train_loader, val_loader, epochs=5, learning_rate=0.01):\n",
    "    def train_res(epoch):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        # progress bar (works in Jupyter notebook too!)\n",
    "        #progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
    "        \n",
    "        progress = enumerate(train_loader)\n",
    "\n",
    "\n",
    "        # ----------------- TRAINING  -------------------- \n",
    "        # set model to training\n",
    "        model.train()\n",
    "\n",
    "        for i, data in progress:\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # training step for single batch\n",
    "            model.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # getting training quality data\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "\n",
    "            # updating progress bar\n",
    "            #progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "\n",
    "        # releasing unceseccary memory in GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # ----------------- VALIDATION  ----------------- \n",
    "        val_losses = 0\n",
    "        precision, recall, f1, accuracy = [], [], [], []\n",
    "\n",
    "        # set model to evaluating (testing)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                outputs = model(X) # this get's the prediction from the network\n",
    "\n",
    "                val_losses += criterion(outputs, y)\n",
    "\n",
    "                predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
    "\n",
    "                # calculate P/R/F1/A metrics for batch\n",
    "                for acc, metric in zip((precision, recall, f1, accuracy), \n",
    "                                       (precision_score, recall_score, f1_score, accuracy_score)):\n",
    "                    acc.append(\n",
    "                        calculate_metric(metric, y.cpu(), predicted_classes.cpu())\n",
    "                    )\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
    "            print_scores(precision, recall, f1, accuracy, val_batches)\n",
    "        losses.append(total_loss/batches) # for plotting learning curve\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    start_ts = time.time()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    losses = []\n",
    "    batches = len(train_loader)\n",
    "    val_batches = len(val_loader)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # loop for every epoch (training + evaluation)\n",
    "    for epoch in range(epochs):\n",
    "        train_res(epoch)\n",
    "\n",
    "    print(f\"Training time: {time.time()-start_ts}s\")\n",
    "    return model\n",
    "\n",
    "def evaluate_ResNet(X_train, y_train,num_classes, epochs=5):\n",
    "    X_train = X_train.transpose(0,3,1,2)\n",
    "    train_loader, val_loader = make_data_loaders(X_train, y_train, batch_size=100)\n",
    "    model = TestResNet(num_classes=num_classes)\n",
    "    model = train(model, train_loader, val_loader, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def read_traffic_signs(rootpath, output_path):\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader)\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            im = Image.open(prefix + row[0])\n",
    "            im = im.resize((56, 56))\n",
    "            images.append(np.asarray(im))\n",
    "            #im.save(os.path.join(output_path, \"{}_{}.png\".format(c, row[0][:row[0].index('.')])), \"PNG\")\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "        \n",
    "    return np.asarray(images), np.asarray(labels)\n",
    "\n",
    "def read_traffic_signs_test(rootpath, output_path):\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    prefix = rootpath + '/'\n",
    "    gtFile = open('GT-final_test.csv') # annotations file\n",
    "    gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "    next(gtReader)\n",
    "    # loop over all images in current annotations file\n",
    "    for row in gtReader:\n",
    "        im = Image.open(prefix + row[0])\n",
    "        im = im.resize((56, 56))\n",
    "        #im.save(os.path.join(output_path, \"{}.png\".format(row[0][:row[0].index('.')])), \"PNG\")\n",
    "        images.append(np.asarray(im))\n",
    "        labels.append(int(row[7])) # the 8th column is the label\n",
    "    gtFile.close()\n",
    "        \n",
    "    return np.asarray(images), np.asarray(labels)\n",
    "\n",
    "def eval_data_augmentation(dataset, c, ratio, X_train, y_train, X_test, y_test, num_classes=10, input_dir='input/', evaluation_method=['imbalanced','classic','BAGAN', 'SinGAN'],\n",
    "                          epochs=5):\n",
    "    class_indices = np.where(y_train == c)[0]\n",
    "    if dataset == 'GTSRB':\n",
    "        to_replace = np.random.choice(class_indices, len(class_indices)-ratio, replace=False)\n",
    "    else:\n",
    "        to_replace = np.random.choice(class_indices, int(ratio*class_indices.size), replace=False)\n",
    "    print(len(to_replace))\n",
    "    left = np.setdiff1d(class_indices, to_replace)\n",
    "    shape = X_train.shape\n",
    "    # load real training samples for minor class\n",
    "    input_paths = list(filter(lambda x : x.startswith(f'{dataset}_{c}'), os.listdir(input_dir)))\n",
    "    train_left = np.zeros(shape=(len(left), shape[1], shape[2], shape[3]))\n",
    "    count = 0\n",
    "    for path in input_paths:\n",
    "        im = Image.open(input_dir+path)\n",
    "        im = np.array(im)\n",
    "        if im.shape != (shape[1],shape[2],shape[3]):\n",
    "            im = im[:,:,:-1]\n",
    "        train_left[count] = (im/255)\n",
    "        count += 1\n",
    "        if count == len(left): break\n",
    "            \n",
    "    for method in evaluation_method:\n",
    "        if method == 'imbalanced':\n",
    "            print(f'Evaluating on imbalanced data set on {dataset} class {c} ratio {ratio}')\n",
    "            X_aug = X_train.copy()\n",
    "            y_aug = y_train.copy()\n",
    "            X_aug[left] = train_left\n",
    "            X_aug = np.delete(X_aug, to_replace, axis=0)\n",
    "            y_aug = np.delete(y_aug,to_replace, axis=0)\n",
    "            model = evaluate_ResNet(X_aug, y_aug, num_classes, epochs=epochs)\n",
    "            compute_test(model, X_test, y_test, c)\n",
    "            del X_aug\n",
    "            del y_aug\n",
    "            \n",
    "        if method == 'classic':\n",
    "            print(f'Evaluating on classic augmentation on {dataset} class {c} ratio {ratio}')\n",
    "            X_aug = X_train.copy()\n",
    "            y_aug = y_train.copy()\n",
    "            X_aug[left] = train_left\n",
    "            seq = get_classic_aug()\n",
    "            for i in range(to_replace.size):\n",
    "                X_aug[to_replace[i]] = seq(image = X_aug[left][i % left.size])\n",
    "            print(f'SSIM on {method} augmentation: {av_SSIM(X_aug[to_replace])}')\n",
    "            model = evaluate_ResNet(X_aug, y_aug,num_classes, epochs=epochs)\n",
    "            compute_test(model, X_test, y_test, c)\n",
    "            del X_aug\n",
    "            del y_aug\n",
    "        \n",
    "        if method == 'SinGAN':\n",
    "            print(f'Evaluating on SinGAN augmentation on {dataset} class {c} ratio {ratio}')\n",
    "            if dataset=='MNIST':\n",
    "                simulated_imgs = simulated_imgs = generate_data(dataset,\n",
    "                                                class_label = c, \n",
    "                                                layer_number = 6,\n",
    "                                                additional_scale = 0,\n",
    "                                                generate_size = to_replace.size, \n",
    "                                                model_size=len(left),\n",
    "                                                generate_start_scale=0)\n",
    "            else:\n",
    "                simulated_imgs = simulated_imgs = generate_data(dataset,\n",
    "                                                class_label = c, \n",
    "                                                layer_number = 5,\n",
    "                                                additional_scale = 0,\n",
    "                                                generate_size = to_replace.size, \n",
    "                                                model_size=len(left),\n",
    "                                                generate_start_scale=0)\n",
    "\n",
    "            print(f'SSIM on {method} augmentation: {av_SSIM(simulated_imgs)}')\n",
    "            X_aug = X_train.copy()\n",
    "            y_aug = y_train.copy()\n",
    "            X_aug[left] = train_left\n",
    "            X_aug[to_replace] = simulated_imgs\n",
    "            del simulated_imgs\n",
    "            model = evaluate_ResNet(X_aug, y_aug, num_classes, epochs=epochs)\n",
    "            compute_test(model, X_test, y_test, c)\n",
    "            del X_aug\n",
    "            del y_aug\n",
    "        \n",
    "        if method == 'BAGAN':\n",
    "            print(f'Evaluating on BAGAN augmentation on {dataset} class {c} ratio {ratio}')\n",
    "            bagan_samples = np.zeros(shape=(len(to_replace), shape[1],shape[2],shape[3]))\n",
    "            count = 0\n",
    "            input_dir = f'./BAGANData/{dataset}/{ratio}/'\n",
    "            for path in list(filter(lambda x: x.startswith('simulated'),os.listdir(input_dir))):\n",
    "                im = Image.open(input_dir+path)\n",
    "                im = np.array(im)\n",
    "                bagan_samples[count] = im/255\n",
    "                count += 1\n",
    "                if count  == len(to_replace) : break\n",
    "            print(f'SSIM on {method} augmentation: {av_SSIM(bagan_samples)}')\n",
    "            X_aug = X_train.copy()\n",
    "            y_aug = y_train.copy()\n",
    "            X_aug[left] = train_left\n",
    "            X_aug[to_replace] = bagan_samples\n",
    "            del bagan_samples\n",
    "            model = evaluate_ResNet(X_aug, y_aug, num_classes, epochs=epochs)\n",
    "            compute_test(model, X_test, y_test, c)\n",
    "            del X_aug\n",
    "            del y_aug\n",
    "        \n",
    "        del model\n",
    "\n",
    "def eval_architecture(dataset, c, ratio, X_train, y_train, X_test, y_test, num_classes, \n",
    "                      layer_number, addtional_scale, input_dir='input/', epochs=30):\n",
    "    class_indices = np.where(y_train == c)[0]\n",
    "    if dataset == 'GTSRB':\n",
    "        to_replace = np.random.choice(class_indices, len(class_indices)-ratio, replace=False)\n",
    "    else:\n",
    "        to_replace = np.random.choice(class_indices, int(ratio*class_indices.size), replace=False)\n",
    "    print(len(to_replace))\n",
    "    left = np.setdiff1d(class_indices, to_replace)\n",
    "    shape = X_train.shape\n",
    "    # load real training samples for minor class\n",
    "    input_paths = list(filter(lambda x : x.startswith(f'{dataset}_{c}'), os.listdir(input_dir)))\n",
    "    train_left = np.zeros(shape=(len(left), shape[1], shape[2], shape[3]))\n",
    "    count = 0\n",
    "    for path in input_paths:\n",
    "        im = Image.open(input_dir+path)\n",
    "        im = np.array(im)\n",
    "        if im.shape != (shape[1],shape[2],shape[3]):\n",
    "            im = im[:,:,:-1]\n",
    "        train_left[count] = (im/255)\n",
    "        count += 1\n",
    "        if count == len(left): break\n",
    "\n",
    "\n",
    "        \n",
    "    print(f'Evaluating on SinGAN augmentation on {layer_number}, {additional_scale}')\n",
    "    simulated_imgs = generate_data(dataset,\n",
    "                                    class_label = c, \n",
    "                                    layer_number = layer_number,\n",
    "                                    additional_scale = additional_scale,\n",
    "                                    generate_size = to_replace.size, \n",
    "                                    model_size=len(left),\n",
    "                                    generate_start_scale=0)\n",
    "\n",
    "    print(f'SSIM on augmentation: {av_SSIM(simulated_imgs)}')\n",
    "    X_aug = X_train.copy()\n",
    "    y_aug = y_train.copy()\n",
    "    X_aug[left] = train_left\n",
    "    X_aug[to_replace] = simulated_imgs\n",
    "    del simulated_imgs\n",
    "    model = evaluate_ResNet(X_aug, y_aug, num_classes, epochs=epochs)\n",
    "    compute_test(model, X_test, y_test, c)\n",
    "    del X_aug\n",
    "    del y_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Architecture Hyperparameters on GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_train = 'GTSRB/Training'\n",
    "output_path_train = 'GTSRB/Training_png'\n",
    "\n",
    "if not os.path.isdir(output_path_train):\n",
    "    os.mkdir(output_path_train, 0o666)\n",
    "    \n",
    "X_train_GTSRB, y_train_GTSRB = read_traffic_signs(input_path_train, output_path_train)\n",
    "y_train_GTSRB = y_train_GTSRB.astype(\"int\")\n",
    "X_train_GTSRB = X_train_GTSRB/255\n",
    "print('Training Sample loaded')\n",
    "classindex_sample = np.load(\"GTSRB_index.npy\").flatten()\n",
    "X_train_GTSRB = X_train_GTSRB[classindex_sample]\n",
    "y_train_GTSRB = y_train_GTSRB[classindex_sample]\n",
    "\n",
    "input_path_test = 'GTSRB/Final_Test/Images'\n",
    "output_path_test = 'GTSRB/Final_Test/converted_png'\n",
    "\n",
    "if not os.path.isdir(output_path_test):\n",
    "    os.mkdir(output_path_test, 0o666)\n",
    "\n",
    "X_test_GTSRB, y_test_GTSRB = read_traffic_signs_test(input_path_test, output_path_test)\n",
    "X_test_GTSRB = X_test_GTSRB/255\n",
    "print('Testing Sample loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [13, 15, 33]:\n",
    "    for layer_number, additional_scale in [(6,-2),(5,-1),(5,0),(6,-1)]:\n",
    "        eval_architecture('GTSRB', c, 5, X_train_GTSRB, y_train_GTSRB, X_test_GTSRB, y_test_GTSRB, num_classes=43, \n",
    "                          layer_number=layer_number, addtional_scale=additional_scale, input_dir='input/', epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation On Different Augmentation Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "mndata = MNIST('Data/MNIST')\n",
    "X_train, y_train = mndata.load_training()\n",
    "X_train = np.array(X_train).reshape(60000,28,28)\n",
    "y_train = np.array(y_train)\n",
    "X_test, y_test = mndata.load_testing()\n",
    "X_test = np.array(X_test).reshape(10000,28,28)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "pre_X = X_train\n",
    "X_train = np.zeros(shape=(60000,28,28,3))\n",
    "for i in range(X_train.shape[0]):\n",
    "    img = np.stack((pre_X[i],)*3, axis=-1)\n",
    "    X_train[i] = img/255\n",
    "del pre_X\n",
    "pre_X_test = X_test\n",
    "X_test = np.zeros(shape=(10000,28,28,3))\n",
    "for i in range(X_test.shape[0]):\n",
    "    img = np.stack((pre_X_test[i],)*3, axis=-1)\n",
    "    X_test[i] = img/255\n",
    "del pre_X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=4 #[2,7,4]\n",
    "for ratio in [0.975, 0.99, 0.995]:\n",
    "    eval_data_augmentation('MNIST', c, ratio, X_train, y_train, X_test, y_test, num_classes=10,\n",
    "                      epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_SSIM(X_train[y_train==c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = evaluate_ResNet(X_train, y_train, num_classes = 10, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_test(model, X_test, y_test, c, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_web import cifar10\n",
    "import numpy as np\n",
    "from skimage import io as img\n",
    "\n",
    "X_train_cifar10, y_train_cifar10, X_test_cifar10, y_test_cifar10 = cifar10(path=None)\n",
    "y_train_cifar10 = np.array([np.argmax(a, axis=0) for a in y_train_cifar10])\n",
    "y_test_cifar10 = np.array([np.argmax(a, axis=0) for a in y_test_cifar10])\n",
    "X_train_cifar10 = X_train_cifar10.reshape(50000, 3, 32, 32).transpose(0,2,3,1)\n",
    "X_test_cifar10 = X_test_cifar10.reshape(10000,3,32,32).transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=3 #[8,9,3]\n",
    "av_SSIM(X_train_cifar10[y_train_cifar10==c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = evaluate_ResNet(X_train_cifar10, y_train_cifar10, num_classes=10, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_test(model, X_test_cifar10, y_test_cifar10, c, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_web import cifar10\n",
    "import numpy as np\n",
    "from skimage import io as img\n",
    "\n",
    "X_train_cifar10, y_train_cifar10, X_test_cifar10, y_test_cifar10 = cifar10(path=None)\n",
    "y_train_cifar10 = np.array([np.argmax(a, axis=0) for a in y_train_cifar10])\n",
    "y_test_cifar10 = np.array([np.argmax(a, axis=0) for a in y_test_cifar10])\n",
    "X_train_cifar10 = X_train_cifar10.reshape(50000, 3, 32, 32).transpose(0,2,3,1)\n",
    "X_test_cifar10 = X_test_cifar10.reshape(10000,3,32,32).transpose(0,2,3,1)\n",
    "\n",
    "for ratio in [0.975, 0.99, 0.995]:\n",
    "    eval_data_augmentation('CIFAR10', 3, ratio, X_train_cifar10, y_train_cifar10, X_test_cifar10, y_test_cifar10, num_classes=10,\n",
    "                      epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_cifar10, y_train_cifar10, X_test_cifar10, y_test_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_train = 'GTSRB/Training'\n",
    "output_path_train = 'GTSRB/Training_png'\n",
    "\n",
    "if not os.path.isdir(output_path_train):\n",
    "    os.mkdir(output_path, 0o666)\n",
    "    \n",
    "X_train_GTSRB, y_train_GTSRB = read_traffic_signs(input_path_train, output_path_train)\n",
    "y_train_GTSRB = y_train_GTSRB.astype(\"int\")\n",
    "X_train_GTSRB = X_train_GTSRB/255\n",
    "print('Training Sample loaded')\n",
    "classindex_sample = np.load(\"GTSRB_index.npy\").flatten()\n",
    "X_train_GTSRB = X_train_GTSRB[classindex_sample]\n",
    "y_train_GTSRB = y_train_GTSRB[classindex_sample]\n",
    "\n",
    "input_path_test = 'GTSRB/Final_Test/Images'\n",
    "output_path_test = 'GTSRB/Final_Test/converted_png'\n",
    "\n",
    "if not os.path.isdir(output_path):\n",
    "    os.mkdir(output_path, 0o666)\n",
    "\n",
    "X_test_GTSRB, y_test_GTSRB = read_traffic_signs_test(input_path_test, output_path_test)\n",
    "X_test_GTSRB = X_test_GTSRB/255\n",
    "print('Testing Sample loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 33  #[13,15,33]\n",
    "\n",
    "for ratio in [5, 10, 15]:\n",
    "    eval_data_augmentation('GTSRB', c, ratio, X_train_GTSRB, y_train_GTSRB, X_test_GTSRB, y_test_GTSRB, num_classes=43,\n",
    "                      epochs=30,evaluation_method=['imbalanced','classic','BAGAN','SinGAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = evaluate_ResNet(X_train_GTSRB, y_train_GTSRB, 43, epochs=20)\n",
    "compute_test(model, X_test_GTSRB, y_test_GTSRB, c)\n",
    "#del X_train_GTSRB, y_train_GTSRB, X_test_GTSRB, y_test_GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(y_test_GTSRB==c)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_SSIM(X_train_GTSRB[y_train_GTSRB==c])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
