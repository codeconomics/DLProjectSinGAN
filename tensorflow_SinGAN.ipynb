{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiKM0eFqbKiE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFRqDMV2bKiJ"
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Scale [0,1] to [-1,1] tensor\n",
    "    \"\"\"\n",
    "    out = (x - 0.5) *2\n",
    "    out = tf.clip_by_value(out, -1, 1)\n",
    "    return out\n",
    "\n",
    "def denorm(x):\n",
    "    \"\"\"\n",
    "    Scale [-1,1] to [0,1] tensor\n",
    "    \"\"\"\n",
    "    out = (x + 1) / 2\n",
    "    out = tf.clip_by_value(out, 0, 1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8v2RTDbbKiM"
   },
   "outputs": [],
   "source": [
    "def np_to_tensor(x):\n",
    "    \"\"\"\n",
    "    for a x*y*c [0,255] nparray x, return a transformed s*x*y*c [-1,1] tf.tensor\n",
    "    \"\"\"\n",
    "    x = x[None,:,:,:]/255 #add a sample dimension, scale to [0,1]\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    x = norm(x) #scale to [-1,1]\n",
    "    return x\n",
    "\n",
    "def tensor_to_np(x):\n",
    "    \"\"\"\n",
    "    for a s*x*y*c [-1,1] tensor, generate its x*y*c [0,255] nparray\n",
    "    \"\"\"\n",
    "    x = denorm(x[0]).numpy()*255\n",
    "    x = x.astype(np.uint8)\n",
    "    return x\n",
    "\n",
    "def convert_image_np(inp):\n",
    "    \"\"\"\n",
    "    for a s*x*y*c [-1,1] tensor to a x*y*c [0,1] np.array\n",
    "    \"\"\"\n",
    "    inp = denorm(inp)#[-1,1] to [0,1]\n",
    "    inp = inp.numpy()[0] # add a sample dimension\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_numpy(ts, name, dir):\n",
    "    np.save(f\"{dir}/{name}\", ts.numpy())\n",
    "\n",
    "def load_from_numpy(name, dir):\n",
    "    np_f = np.load(f\"{dir}/{name}\")\n",
    "    return tf.convert_to_tensor(np_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2ZtuV20bKiO"
   },
   "outputs": [],
   "source": [
    "from skimage import io as img\n",
    "def read_image_np(opt):\n",
    "    \"\"\"\n",
    "    read the image and return the np.array form\n",
    "    \"\"\"\n",
    "    x = img.imread(f'{opt.input_dir}/{opt.input_name}')\n",
    "    return x\n",
    "\n",
    "def read_image_tensor(opt):\n",
    "    \"\"\"\n",
    "    read the image and return the [-1,1] tensor form\n",
    "    \"\"\"\n",
    "    #read the image defined in opt then return to a tensor\n",
    "    x = img.imread(f'{opt.input_dir}/{opt.input_name}')\n",
    "    return np_to_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vars(G, z, noiseamp, real, opt, scale_num):\n",
    "    G.save(f\"{opt.out_}/Gs_{scale_num}.tf\")\n",
    "    save_as_numpy(z, f\"Zs_{scale_num}\", opt.out_)\n",
    "    save_as_numpy(real, f\"reals_{scale_num}\", opt.out_)\n",
    "    np.save(f\"{opt.out_}/NoiseAmp_{scale_num}\", np.array(noiseamp))\n",
    "\n",
    "def load_trained_pyramid(opt):\n",
    "    \"\"\"\n",
    "    load every model trained \n",
    "    \"\"\"\n",
    "    #get the direction and\n",
    "    dir = generate_dir2save(opt)\n",
    "    Gs = []\n",
    "    Zs = []\n",
    "    reals = []\n",
    "    NoiseAmp = []\n",
    "    print(dir)\n",
    "    if(os.path.exists(dir)):\n",
    "        i = 0\n",
    "        while(True):\n",
    "            try:\n",
    "                Zs.append(load_from_numpy(f\"Zs_{i}.npy\", dir))\n",
    "                reals.append(load_from_numpy(f\"reals_{i}.npy\", dir))\n",
    "                model = load_model(f'{dir}/Gs_{i}.tf')\n",
    "                Gs.append(model)\n",
    "                NoiseAmp.append(np.load(f'{dir}/NoiseAmp_{i}.npy').item())\n",
    "                i += 1\n",
    "            except:\n",
    "                break\n",
    "        return Gs,Zs,reals,NoiseAmp\n",
    "    else:\n",
    "        raise NameError('no appropriate trained model is exist, 4 empty lists are returned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAF91FPabKid"
   },
   "outputs": [],
   "source": [
    "def generate_dir2save(opt):\n",
    "    \"\"\"\n",
    "    manually defined directory\n",
    "    \"\"\"\n",
    "    dir2save = f'{opt.out}/{opt.input_name}/layer={opt.num_layer}, additional_scale={bool(opt.additional_scale)}, iteration={opt.niter}, scale_factor={opt.scale_factor_init}, alpha={opt.alpha}'\n",
    "    return dir2save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qj32IQ1NbKik"
   },
   "outputs": [],
   "source": [
    "def generate_noise(size,num_samp=1):\n",
    "    \"\"\"\n",
    "    generate the noise of size = [width, height, channel]\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([num_samp, size[0], size[1], size[2]])\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IS2xxWFbKin"
   },
   "outputs": [],
   "source": [
    "def save_networks(netG,netD,z,opt):\n",
    "    \"\"\"\n",
    "    save model netG, netD and z into the directory specified by opt\n",
    "    \"\"\"\n",
    "    #to tf mode here\n",
    "    netG.save(f'{opt.outf}/netG.tf')\n",
    "    netD.save(f'{opt.outf}/netD.tf')\n",
    "    netG.save_weights(f'{opt.outf}/netGweights.tf')\n",
    "    netD.save_weights(f'{opt.outf}/netDweights.tf')\n",
    "    save_as_numpy(z, 'z_opt', opt.outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reals_pyramid(real,opt):\n",
    "    \"\"\"\n",
    "    from image real, generate reals, the whole list of different scale image\n",
    "    \"\"\"\n",
    "    reals = [imresize_scale_tensor(real,math.pow(opt.scale_factor, opt.stop_scale-i)) for i in range(0,opt.stop_scale+1)]\n",
    "    return reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smCvLB_rbKir"
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #load, input, save configurations:\n",
    "    parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "    parser.add_argument('--nc_z',type=int,help='noise # channels',default=3)\n",
    "    parser.add_argument('--nc_im',type=int,help='image # channels',default=3)\n",
    "    parser.add_argument('--out',help='output folder',default='Output')\n",
    "        \n",
    "    #networks hyper parameters:\n",
    "    parser.add_argument('--nfc', type=int, default=32)\n",
    "    parser.add_argument('--min_nfc', type=int, default=32)\n",
    "    parser.add_argument('--ker_size',type=int,help='kernel size',default=3)\n",
    "    parser.add_argument('--num_layer',type=int,help='number of layers',default=5)\n",
    "    parser.add_argument('--stride',help='stride',default=1)\n",
    "    parser.add_argument('--padd_size',type=int,help='net pad size',default=0)#math.floor(opt.ker_size/2)\n",
    "        \n",
    "    #pyramid parameters:\n",
    "    parser.add_argument('--scale_factor',type=float,help='pyramid scale factor',default=0.75)#pow(0.5,1/6))\n",
    "    parser.add_argument('--noise_amp',type=float,help='addative noise cont weight',default=0.1)\n",
    "    parser.add_argument('--min_size',type=int,help='image minimal size at the coarser scale',default=25)\n",
    "    parser.add_argument('--max_size', type=int,help='image minimal size at the coarser scale', default=250)\n",
    "\n",
    "    #optimization hyper parameters:\n",
    "    parser.add_argument('--niter', type=int, default=2000, help='number of epochs to train per scale')\n",
    "    parser.add_argument('--gamma',type=float,help='scheduler gamma',default=0.1)\n",
    "    parser.add_argument('--lr_g', type=float, default=0.0005, help='learning rate, default=0.0005')\n",
    "    parser.add_argument('--lr_d', type=float, default=0.0005, help='learning rate, default=0.0005')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "    parser.add_argument('--lambda_grad',type=float, help='gradient penelty weight',default=0.1)\n",
    "    parser.add_argument('--alpha',type=float, help='reconstruction loss weight',default=5)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def post_config(opt):\n",
    "    \"\"\"\n",
    "    the additional specification in the opt\n",
    "    \"\"\"\n",
    "    # init fixed parameters\n",
    "    opt.niter_init = opt.niter\n",
    "    opt.noise_amp_init = opt.noise_amp\n",
    "    opt.nfc_init = opt.nfc\n",
    "    opt.min_nfc_init = opt.min_nfc\n",
    "    opt.scale_factor_init = opt.scale_factor\n",
    "    random.seed(opt.manualSeed)\n",
    "    tf.random.set_seed(opt.manualSeed)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfpN7vAEbKi2"
   },
   "outputs": [],
   "source": [
    "def contributions(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n",
    "    \"\"\"\n",
    "    support function of imresize_in,calculates a set of 'filters' and a set of field_of_view that will later on be applied\n",
    "    such that each position from the field_of_view will be multiplied with a matching filter from the  'weights' based on \n",
    "    the interpolation method and the distance of the sub-pixel location from the pixel centers around it. \n",
    "    This is only done for one dimension of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    # When anti-aliasing is activated (default and only for downscaling) the receptive field is stretched to size of\n",
    "    # 1/sf. this means filtering is more 'low-pass filter'.\n",
    "    fixed_kernel = (lambda arg: scale * kernel(scale * arg)) if antialiasing else kernel\n",
    "    kernel_width *= 1.0 / scale if antialiasing else 1.0\n",
    "\n",
    "    # These are the coordinates of the output image\n",
    "    out_coordinates = np.arange(1, out_length+1)\n",
    "\n",
    "    # These are the matching positions of the output-coordinates on the input image coordinates.\n",
    "    # Best explained by example: say we have 4 horizontal pixels for HR and we downscale by SF=2 and get 2 pixels:\n",
    "    # [1,2,3,4] -> [1,2]. Remember each pixel number is the middle of the pixel.\n",
    "    # The scaling is done between the distances and not pixel numbers (the right boundary of pixel 4 is transformed to\n",
    "    # the right boundary of pixel 2. pixel 1 in the small image matches the boundary between pixels 1 and 2 in the big\n",
    "    # one and not to pixel 2. This means the position is not just multiplication of the old pos by scale-factor).\n",
    "    # So if we measure distance from the left border, middle of pixel 1 is at distance d=0.5, border between 1 and 2 is\n",
    "    # at d=1, and so on (d = p - 0.5).  we calculate (d_new = d_old / sf) which means:\n",
    "    # (p_new-0.5 = (p_old-0.5) / sf)     ->          p_new = p_old/sf + 0.5 * (1-1/sf)\n",
    "    match_coordinates = 1.0 * out_coordinates / scale + 0.5 * (1 - 1.0 / scale)\n",
    "\n",
    "    # This is the left boundary to start multiplying the filter from, it depends on the size of the filter\n",
    "    left_boundary = np.floor(match_coordinates - kernel_width / 2)\n",
    "\n",
    "    # Kernel width needs to be enlarged because when covering has sub-pixel borders, it must 'see' the pixel centers\n",
    "    # of the pixels it only covered a part from. So we add one pixel at each side to consider (weights can zeroize them)\n",
    "    expanded_kernel_width = np.ceil(kernel_width) + 2\n",
    "\n",
    "    # Determine a set of field_of_view for each each output position, these are the pixels in the input image\n",
    "    # that the pixel in the output image 'sees'. We get a matrix whos horizontal dim is the output pixels (big) and the\n",
    "    # vertical dim is the pixels it 'sees' (kernel_size + 2)\n",
    "    field_of_view = np.squeeze(np.uint(np.expand_dims(left_boundary, axis=1) + np.arange(expanded_kernel_width) - 1))\n",
    "\n",
    "    # Assign weight to each pixel in the field of view. A matrix whos horizontal dim is the output pixels and the\n",
    "    # vertical dim is a list of weights matching to the pixel in the field of view (that are specified in\n",
    "    # 'field_of_view')\n",
    "    weights = fixed_kernel(1.0 * np.expand_dims(match_coordinates, axis=1) - field_of_view - 1)\n",
    "\n",
    "    # Normalize weights to sum up to 1. be careful from dividing by 0\n",
    "    sum_weights = np.sum(weights, axis=1)\n",
    "    sum_weights[sum_weights == 0] = 1.0\n",
    "    weights = 1.0 * weights / np.expand_dims(sum_weights, axis=1)\n",
    "\n",
    "    # We use this mirror structure as a trick for reflection padding at the boundaries\n",
    "    mirror = np.uint(np.concatenate((np.arange(in_length), np.arange(in_length - 1, -1, step=-1))))\n",
    "    field_of_view = mirror[np.mod(field_of_view, mirror.shape[0])]\n",
    "\n",
    "    # Get rid of  weights and pixel positions that are of zero weight\n",
    "    non_zero_out_pixels = np.nonzero(np.any(weights, axis=0))\n",
    "    weights = np.squeeze(weights[:, non_zero_out_pixels])\n",
    "    field_of_view = np.squeeze(field_of_view[:, non_zero_out_pixels])\n",
    "\n",
    "    # Final products are the relative positions and the matching weights, both are output_size X fixed_kernel_size\n",
    "    return weights, field_of_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipcNoFU3bKi5"
   },
   "outputs": [],
   "source": [
    "def resize_along_dim(im, dim, weights, field_of_view):\n",
    "    \"\"\"\n",
    "    support function of imresize_in, resize im along the dim given\n",
    "    \"\"\"\n",
    "    \n",
    "    # To be able to act on each dim, we swap so that dim 0 is the wanted dim to resize\n",
    "    tmp_im = np.swapaxes(im, dim, 0)\n",
    "\n",
    "    # We add singleton dimensions to the weight matrix so we can multiply it with the big tensor we get for\n",
    "    # tmp_im[field_of_view.T], (bsxfun style)\n",
    "    weights = np.reshape(weights.T, list(weights.T.shape) + (np.ndim(im) - 1) * [1])\n",
    "\n",
    "    # This is a bit of a complicated multiplication: tmp_im[field_of_view.T] is a tensor of order image_dims+1.\n",
    "    # for each pixel in the output-image it matches the positions the influence it from the input image (along 1 dim\n",
    "    # only, this is why it only adds 1 dim to the shape). We then multiply, for each pixel, its set of positions with\n",
    "    # the matching set of weights. we do this by this big tensor element-wise multiplication (MATLAB bsxfun style:\n",
    "    # matching dims are multiplied element-wise while singletons mean that the matching dim is all multiplied by the\n",
    "    # same number\n",
    "    tmp_out_im = np.sum(tmp_im[field_of_view.T] * weights, axis=0)\n",
    "\n",
    "    # Finally we swap back the axes to the original order\n",
    "    return np.swapaxes(tmp_out_im, dim, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yEPUfAm1bKi8"
   },
   "outputs": [],
   "source": [
    "# interpolation methods. x is the distance from the left pixel center\n",
    "def cubic(x):\n",
    "    absx = np.abs(x)\n",
    "    absx2 = absx ** 2\n",
    "    absx3 = absx ** 3\n",
    "    return ((1.5*absx3 - 2.5*absx2 + 1) * (absx <= 1) +\n",
    "            (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * ((1 < absx) & (absx <= 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrvx8WWpbKi-"
   },
   "outputs": [],
   "source": [
    "def imresize_in(im, scale_factor=None):\n",
    "    \"\"\"\n",
    "    complete procedure of resizing\n",
    "    \"\"\"\n",
    "    # First standardize values and fill missing arguments (if needed) by deriving scale from output shape or vice versa\n",
    "    input_shape = im.shape\n",
    "    \n",
    "    scale_factor = [scale_factor, scale_factor]\n",
    "    scale_factor = list(scale_factor) + ([1] * (len(input_shape) - len(scale_factor)))\n",
    "\n",
    "    # Dealing with missing output-shape. calculating according to scale-factor\n",
    "    output_shape = np.uint(np.ceil(np.array(input_shape) * np.array(scale_factor)))\n",
    "    \n",
    "    # Choose interpolation method, each method has the matching kernel size\n",
    "    method = cubic \n",
    "    kernel_width = 4.0\n",
    "    \n",
    "    # Antialiasing is only used when downscaling\n",
    "    antialiasing = (scale_factor[0] < 1)\n",
    "\n",
    "    # Sort indices of dimensions according to scale of each dimension. since we are going dim by dim this is efficient\n",
    "    dims_sorted = np.argsort(np.array(scale_factor)).tolist()\n",
    "\n",
    "    # Iterate over dimensions to calculate local weights for resizing and resize each time in one direction\n",
    "    out_im = np.copy(im)\n",
    "    for dim in dims_sorted:\n",
    "        # No point doing calculations for scale-factor 1. nothing will happen anyway\n",
    "        if scale_factor[dim] == 1.0:\n",
    "            continue\n",
    "\n",
    "        # for each coordinate (along 1 dim), calculate which coordinates in the input image affect its result and the\n",
    "        # weights that multiply the values there to get its result.\n",
    "        weights, field_of_view = contributions(im.shape[dim], output_shape[dim], scale_factor[dim],\n",
    "                                               method, kernel_width, antialiasing)\n",
    "\n",
    "        # Use the affecting position values and the set of weights to calculate the result of resizing along this 1 dim\n",
    "        out_im = resize_along_dim(out_im, dim, weights, field_of_view)\n",
    "\n",
    "    return out_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owwLaY_kbKjA"
   },
   "outputs": [],
   "source": [
    "def imresize_scale_tensor(im,scale_factor): # original imresize\n",
    "    \"\"\"\n",
    "    the wrapper of imresize_in for tensors, rescale a tensor by scale factor = scale\n",
    "    \"\"\"\n",
    "    #change image from tensor to numpy array,  \n",
    "    im = tensor_to_np(im)\n",
    "    #resize it with scale,\n",
    "    im = imresize_in(im, scale_factor=scale_factor)\n",
    "    #then get back to tensor\n",
    "    im = np_to_tensor(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuY0r4vrbKjD"
   },
   "outputs": [],
   "source": [
    "def zero_pad(tensor, pad_width):\n",
    "    \"\"\"\n",
    "    give a zero padding to im of width = pad_width\n",
    "    can be applied to both 3D and 4D tensor\n",
    "    \"\"\"\n",
    "    width = tensor.shape[len(tensor.shape)-3]\n",
    "    height = tensor.shape[len(tensor.shape)-2]\n",
    "    return tf.image.resize_with_crop_or_pad(tensor, \n",
    "                                            width + 2 * pad_width,\n",
    "                                            height + 2 * pad_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imYYW5ZZbKjO"
   },
   "outputs": [],
   "source": [
    "def generator(opt, pad_noise):\n",
    "    N = opt.nfc\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(filters = max(N,opt.min_nfc),#output\n",
    "                            input_shape= (opt.real_x+2*pad_noise, opt.real_y+2*pad_noise, opt.nc_im),\n",
    "                            padding = \"valid\",\n",
    "                            kernel_size = opt.ker_size,\n",
    "                            strides = 1,\n",
    "                            kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
    "                           ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    for i in range(opt.num_layer - 2):\n",
    "        N = int(opt.nfc/pow(2,(i+1)))\n",
    "        model.add(layers.Conv2D(filters = max(N,opt.min_nfc), \n",
    "                                                  padding = \"valid\",\n",
    "                                                  kernel_size = opt.ker_size,\n",
    "                                                  strides = 1,\n",
    "                                                  kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(filters = opt.nc_im, \n",
    "                            padding = \"valid\",\n",
    "                            kernel_size = opt.ker_size,\n",
    "                            strides = 1,\n",
    "                            activation='tanh',\n",
    "                            kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)))\n",
    "    return model\n",
    "\n",
    "# class generator(tf.keras.Model):\n",
    "#     def __init__(self,opt):\n",
    "#         super(generator, self).__init__()\n",
    "#         N = opt.nfc\n",
    "#         self.conv_first = layers.Conv2D(\n",
    "#                                 filters = max(N,opt.min_nfc),#output\n",
    "#                                 input_shape= (opt.real_x, opt.real_y, opt.nc_im),\n",
    "#                                 padding = \"valid\",\n",
    "#                                 kernel_size = opt.ker_size,\n",
    "#                                 strides = 1,\n",
    "#                                 kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
    "#                                )\n",
    "#         self.batch_first = layers.BatchNormalization()\n",
    "#         self.relu_first = layers.LeakyReLU(alpha = 0.2)\n",
    "#         self.body_layers = []\n",
    "#         for i in range(opt.num_layer - 2):\n",
    "#             N = int(opt.nfc/pow(2,(i+1)))\n",
    "#             self.body_layers.append(layers.Conv2D(filters = max(N,opt.min_nfc), \n",
    "#                                                   padding = \"valid\",\n",
    "#                                                   kernel_size = opt.ker_size,\n",
    "#                                                   strides = 1,\n",
    "#                                                   kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)))\n",
    "#             self.body_layers.append(layers.BatchNormalization())\n",
    "#             self.body_layers.append(layers.LeakyReLU(alpha = 0.2))\n",
    "#         self.end_layer = layers.Conv2D(filters = opt.nc_im, \n",
    "#                                   padding = \"valid\",\n",
    "#                                   kernel_size = opt.ker_size,\n",
    "#                                   strides = 1,\n",
    "#                                   activation='tanh',\n",
    "#                                   kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None))\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         x = self.conv_first(x)\n",
    "#         x = self.batch_first(x)\n",
    "#         x = self.relu_first(x)\n",
    "#         for i in self.body_layers:\n",
    "#             x = i(x)\n",
    "#         x = self.end_layer(x)\n",
    "#         #if the shape is different, choose y in the \"middle\"\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aj3gVglgbKjQ"
   },
   "outputs": [],
   "source": [
    "def discriminator(opt):\n",
    "    N = opt.nfc\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(filters = max(N,opt.min_nfc),#output\n",
    "                                        input_shape= (opt.real_x, opt.real_y, opt.nc_im),\n",
    "                                        padding = \"valid\",\n",
    "                                        kernel_size = opt.ker_size,\n",
    "                                        strides = 1,\n",
    "                                        bias_initializer='zeros',\n",
    "                                        kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
    "                                        ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    for i in range(opt.num_layer - 2):\n",
    "        N = int(opt.nfc/pow(2,(i+1)))\n",
    "        model.add(layers.Conv2D(filters = max(N,opt.min_nfc), \n",
    "                                    padding = \"valid\",\n",
    "                                    kernel_size = opt.ker_size,\n",
    "                                    strides = 1,\n",
    "                                    kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(filters = 1, \n",
    "                            padding = \"valid\",\n",
    "                            kernel_size = opt.ker_size,\n",
    "                            strides = 1,\n",
    "                            kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)))\n",
    "    return model\n",
    "\n",
    "\n",
    "# class discriminator(tf.keras.Model):\n",
    "#     def __init__(self,opt):\n",
    "#         super(discriminator, self).__init__()\n",
    "#         N = opt.nfc\n",
    "#         #self.input_layer = Input(shape=(opt.real_x, opt.real_y, opt.nc_im))\n",
    "#         self.conv_first = layers.Conv2D(filters = max(N,opt.min_nfc),#output\n",
    "#                                         input_shape= (opt.real_x, opt.real_y, opt.nc_im),\n",
    "#                                         padding = \"valid\",\n",
    "#                                         kernel_size = opt.ker_size,\n",
    "#                                         strides = 1,\n",
    "#                                         bias_initializer='zeros',\n",
    "#                                         kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
    "#                                         )\n",
    "#         self.batch_first = layers.BatchNormalization()\n",
    "#         self.relu_first = layers.LeakyReLU(alpha = 0.2)\n",
    "#         self.body_layers = []\n",
    "#         for i in range(opt.num_layer - 2):\n",
    "#             N = int(opt.nfc/pow(2,(i+1)))\n",
    "#             self.body_layers.append(layers.Conv2D(filters = max(N,opt.min_nfc), \n",
    "#                                     padding = \"valid\",\n",
    "#                                     kernel_size = opt.ker_size,\n",
    "#                                     strides = 1,\n",
    "#                                     kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)))\n",
    "#             self.body_layers.append(layers.BatchNormalization())\n",
    "#             self.body_layers.append(layers.LeakyReLU(alpha = 0.2))\n",
    "\n",
    "#         self.conv_last = layers.Conv2D(filters = 1, \n",
    "#                           padding = \"valid\",\n",
    "#                           kernel_size = opt.ker_size,\n",
    "#                           strides = 1,\n",
    "#                           activation='tanh',\n",
    "#                           kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None))\n",
    "\n",
    "        \n",
    "#     def call(self, x):\n",
    "        \n",
    "#         x = self.conv_first(x)\n",
    "#         x = self.batch_first(x)\n",
    "#         x = self.relu_first(x)\n",
    "#         for i in self.body_layers:\n",
    "#             x = i(x)\n",
    "#         x = self.conv_last(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qOfw9A3GbKjS"
   },
   "outputs": [],
   "source": [
    "def init_models(opt, pad_noise):\n",
    "    \"\"\"\n",
    "    initialize a pair of generator and discriminator\n",
    "    \"\"\"\n",
    "    #generator initialization:\n",
    "    netG = generator(opt, pad_noise)\n",
    "    #discriminator initialization:\n",
    "    netD = discriminator(opt)\n",
    "    #note: both model has fixed initializer\n",
    "    return netG,netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_process(netG, image_input, prev_image):\n",
    "    \"\"\"\n",
    "    to keap a linear sequential model, append the prev_image afterwards\n",
    "    \"\"\"\n",
    "    res = netG(tf.stop_gradient(image_input), training=False)\n",
    "    width = int((prev_image.shape[1]-res.shape[1])/2)\n",
    "    result = res + prev_image[:,width:(prev_image.shape[1]-width),width:(prev_image.shape[2]-width),:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qExXR6QSbKjU"
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data, LAMBDA):\n",
    "    \"\"\"\n",
    "    calculate the gradient penalty in order to improve the performance of Wasserstein GANs\n",
    "    from  https://arxiv.org/pdf/1704.00028.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    #a N(0,1) variable in correct shape\n",
    "    alpha = tf.random.uniform(shape = [1,1])\n",
    "    alpha = tf.broadcast_to(alpha, real_data.shape)\n",
    "\n",
    "    #linear interpolation of these two stuff, make it a trainable tensor\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = tf.Variable(interpolates, trainable=True)\n",
    "\n",
    "    with tf.GradientTape() as penalty_tape:\n",
    "        penalty_tape.watch(interpolates)\n",
    "        disc_interpolates = netD(interpolates)\n",
    "\n",
    "    #generate the autograd instance\n",
    "    gradients = penalty_tape.gradient(target=disc_interpolates,\n",
    "                                      sources=interpolates,\n",
    "                                      output_gradients=tf.ones_like(disc_interpolates))\n",
    "\n",
    "    gradient_penalty = LAMBDA * tf.math.reduce_mean((tf.norm(gradients, axis=1) -1) ** 2)\n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0ytPUVOwZcu"
   },
   "outputs": [],
   "source": [
    "def adjust_scales2image(real_,opt):\n",
    "    #calculate the number of scale\n",
    "    opt.num_scales = math.ceil((math.log(math.pow(opt.min_size / (min(real_.shape[1], real_.shape[2])), 1), opt.scale_factor_init))) \\\n",
    "                     + 1 * opt.scale_plus1 + 1 * opt.additional_scale # newly added here\n",
    "    \n",
    "    opt.stop_scale = opt.num_scales - math.ceil(math.log(min([opt.max_size, max([real_.shape[1], real_.shape[2]])]) / max([real_.shape[1], real_.shape[2]]),opt.scale_factor_init))\n",
    "    opt.scale1 = min(opt.max_size / max([real_.shape[1], real_.shape[2]]),1)  # min(250/max([real_.shape[0],real_.shape[1]]),1)\n",
    "    real_shape = imresize_scale_tensor(real_, opt.scale1).shape\n",
    "    opt.scale_factor = math.pow(opt.min_size/(min(real_shape[1],real_shape[2])),1/(opt.stop_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRlT6AMObKjW"
   },
   "outputs": [],
   "source": [
    "def draw_concat(Gs,Zs,reals,NoiseAmp,in_s,mode,pad_noise,pad_image,opt):\n",
    "    \"\"\"\n",
    "    Given current information of all the previous scales, generate the image cut \n",
    "    pass to the next layer (see Figure 4 in the article)\n",
    "    \"\"\"\n",
    "    #if it's the first scale return in_s\n",
    "    G_z = in_s\n",
    "    #if it's not the first scale\n",
    "    if len(Gs) > 0:\n",
    "        #rand mode: draw with random noise\n",
    "        if mode == 'rand':\n",
    "            count = 0\n",
    "            #for each scale, note that real_next from reals[1:] is the next layer size\n",
    "            for G,Z_opt,real_curr,real_next,noise_amp in zip(Gs,Zs,reals,reals[1:],NoiseAmp):\n",
    "                \n",
    "                #for the first scale\n",
    "                if count == 0:\n",
    "                    #generate a 1 channel noise with 2 pad_noise smaller than Z_opt on each axis\n",
    "                    #expand it to the original shape and 3 channels\n",
    "                    z = generate_noise([Z_opt.shape[1] - 2 * pad_noise, Z_opt.shape[2] - 2 * pad_noise, 1])\n",
    "                    z = tf.broadcast_to(z, (1,z.shape[1], z.shape[2],3))\n",
    "                else:\n",
    "                    #generate a noise with 2 pad_noise smaller than Z_opt on each axis and channel specified\n",
    "                    z = generate_noise([Z_opt.shape[1] - 2 * pad_noise, Z_opt.shape[2] - 2 * pad_noise,opt.nc_z])\n",
    "                \n",
    "                #give it a additional zero pad\n",
    "                z = zero_pad(z, pad_noise)\n",
    "                \n",
    "                #cut in_s into the shape of current scale real image,give it a image pad\n",
    "                G_z = G_z[:,0:real_curr.shape[1],0:real_curr.shape[2],:]\n",
    "                G_z = zero_pad(G_z, pad_image)\n",
    "                \n",
    "                #have the noise scaled and add it to the cut\n",
    "                z_in = noise_amp*z+G_z\n",
    "                #generate the part to the generator, then added with original image(see the article)\n",
    "                G_z = G_process(G,z_in, G_z)\n",
    "                \n",
    "                #resize it by 1/opt.scale_factor (scale up)\n",
    "                G_z = imresize_scale_tensor(G_z,1/opt.scale_factor)\n",
    "                \n",
    "                #cut it into the shape fit real graph in the next scale(in case there's rounding issue)\n",
    "                G_z = G_z[:,0:real_next.shape[1],0:real_next.shape[2],:]\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "        #fix mode: draw with noise Z_opt\n",
    "        if mode == 'fix':\n",
    "            #similarly\n",
    "            for G,Z_opt,real_curr,real_next,noise_amp in zip(Gs,Zs,reals,reals[1:],NoiseAmp):\n",
    "                #cut in_s into the shape of current layer real image\n",
    "                G_z = G_z[:, 0:real_curr.shape[1], 0:real_curr.shape[2],:]\n",
    "                #give it a image pad\n",
    "                G_z = zero_pad(G_z, pad_image)\n",
    "                #have the Z_opt noise scaled and add it to the cut\n",
    "                z_in = noise_amp*Z_opt+G_z\n",
    "                #generate the part to the generator, then added with original image(see the article)\n",
    "                G_z = G_process(G,z_in, G_z)\n",
    "                #resize it by 1/opt.scale_factor (scale up)\n",
    "                G_z = imresize_scale_tensor(G_z,1/opt.scale_factor)\n",
    "                #cut it into the shape of real graph in the next layer (in case there's rounding issue)\n",
    "                G_z = G_z[:,0:real_next.shape[1],0:real_next.shape[2],:]\n",
    "    return G_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VptHpymkbKjY"
   },
   "outputs": [],
   "source": [
    "def train_single_scale(reals,Gs,Zs,in_s,NoiseAmp,opt, nfc_prev, scale_num):\n",
    "    \"\"\"\n",
    "    given a pair of discriminator and generator, train the new scale with all known\n",
    "    data from previous scales\n",
    "    \"\"\"\n",
    "    #get the corresponding real image\n",
    "    real = reals[len(Gs)]\n",
    "    #get its shape\n",
    "    opt.real_x = real.shape[1]\n",
    "    opt.real_y = real.shape[2]\n",
    "    pad_noise = int(((opt.ker_size - 1) * opt.num_layer) / 2)\n",
    "    pad_image = int(((opt.ker_size - 1) * opt.num_layer) / 2)\n",
    "    z_opt = tf.fill([1,opt.real_x + 2 * pad_noise,opt.real_y + 2 * pad_noise,opt.nc_z], 0.)\n",
    "    alpha = opt.alpha\n",
    "    \n",
    "    #init the discriminator and generator\n",
    "    netG,netD = init_models(opt, pad_noise)\n",
    "    #if the scales are not \"far\" from each other, warm-start with the last model\n",
    "    if (nfc_prev == opt.nfc):\n",
    "        netG.load_weights(f'{opt.out_}/{scale_num-1}/netGweights.tf')\n",
    "        netD.load_weights(f'{opt.out_}/{scale_num-1}/netDweights.tf')\n",
    "\n",
    "    # setup optimizer & scheduler\n",
    "    decay_lr_D = PiecewiseConstantDecay(boundaries=[1600],values=[opt.lr_d, opt.lr_d*opt.gamma])\n",
    "    decay_lr_G = PiecewiseConstantDecay(boundaries=[1600],values=[opt.lr_g, opt.lr_g*opt.gamma])\n",
    "    optimizerD = tf.keras.optimizers.Adam(learning_rate=decay_lr_D, beta_1=opt.beta1, beta_2=0.999)\n",
    "    optimizerG = tf.keras.optimizers.Adam(learning_rate=decay_lr_G, beta_1=opt.beta1, beta_2=0.999)\n",
    "    \n",
    "    #build the model\n",
    "    noise = zero_pad(tf.fill([1,opt.real_x, opt.real_y, opt.nc_z], 1.0), pad_noise)\n",
    "    fake = netG.predict(tf.stop_gradient(noise))\n",
    "    netD.predict(tf.stop_gradient(fake))\n",
    "    del noise, fake\n",
    "    \n",
    "    #for niter's loop\n",
    "    for epoch in tqdm(range(opt.niter), desc = f\"scale {len(Gs)}\", leave = False):\n",
    "        #if there's no G in Gs\n",
    "        if (Gs == []):\n",
    "            #generate a normal noise and a normal z_opt, give it pad\n",
    "            z_opt = generate_noise([opt.real_x,opt.real_y,1])\n",
    "            z_opt = zero_pad(tf.broadcast_to(z_opt,(1,opt.real_x,opt.real_y,3)), pad_noise)\n",
    "            noise_ = generate_noise([opt.real_x,opt.real_y,1])\n",
    "            noise_ = zero_pad(tf.broadcast_to(noise_,(1,opt.real_x,opt.real_y,3)), pad_noise)\n",
    "        else:\n",
    "            #noise is normal, then padded\n",
    "            noise_ = generate_noise([opt.real_x, opt.real_y, opt.nc_z])\n",
    "            noise_ = zero_pad(noise_, pad_noise)\n",
    "        #noise_list = []\n",
    "        #Update Discrimiator in Wasserstein GANs: maximize D(x) + D(G(z))\n",
    "        for j in range(3):\n",
    "            #for the first step in this scale\n",
    "            if (j==0) & (epoch == 0):\n",
    "                #if it's the first scale (very first step)\n",
    "                if (Gs == []):\n",
    "                    #set prev,z_prev and in_s as all 0\n",
    "                    prev = tf.fill((1,opt.real_x,opt.real_y,opt.nc_z), 0.)\n",
    "                    z_prev = tf.fill((1,opt.real_x,opt.real_y,opt.nc_z), 0.)\n",
    "                    in_s = prev\n",
    "                    \n",
    "                    #pad prev as a image, pad z_prev as a noise\n",
    "                    prev = zero_pad(prev, pad_image)\n",
    "                    z_prev = zero_pad(z_prev, pad_noise)\n",
    "                    \n",
    "                    #set amplify coefficient as 1\n",
    "                    opt.noise_amp = 1\n",
    "                    \n",
    "                #if the first step but not the first scale\n",
    "                else:\n",
    "                    #draw the random noise concate,add the padding as a image\n",
    "                    prev = draw_concat(Gs,Zs,reals,NoiseAmp,in_s,'rand',pad_noise,pad_image,opt)\n",
    "                    prev = zero_pad(prev, pad_image)\n",
    "                    \n",
    "                    #draw the fix noise concate\n",
    "                    z_prev = draw_concat(Gs,Zs,reals,NoiseAmp,in_s,'fix',pad_noise,pad_image,opt)\n",
    "                    \n",
    "                    #use the RMSE between the real image and the fix noise concate to update the amplify coefficient \n",
    "                    MSE = tf.keras.losses.MeanSquaredError()\n",
    "                    RMSE = tf.math.sqrt(MSE(real, z_prev))\n",
    "                    opt.noise_amp = opt.noise_amp_init*RMSE\n",
    "                    \n",
    "                    #padded the fix noise concate after RMSE is calculated\n",
    "                    z_prev = zero_pad(z_prev,pad_image)\n",
    "                    \n",
    "            #if it's not the first step in this scale\n",
    "            else:\n",
    "                #draw random noise concatenation, pad it as an image\n",
    "                prev = draw_concat(Gs,Zs,reals,NoiseAmp,in_s,'rand',pad_noise,pad_image,opt)\n",
    "                prev = zero_pad(prev, pad_image)\n",
    "\n",
    "            #if it's the first scale\n",
    "            if (Gs == []):\n",
    "                #make the noise generated as the initial noise\n",
    "                noise = noise_\n",
    "\n",
    "            else:\n",
    "                #if not, add scaled new noise into the \"scaled-accumulation\" of noise\n",
    "                noise = opt.noise_amp*noise_+prev\n",
    "                \n",
    "            #noise_list.append(tf.stop_gradient(noise))\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as disc_tape:\n",
    "                disc_tape.watch(netD.trainable_variables)\n",
    "                \n",
    "                #generate a result with real image, as the discriminator, the error for real should be -a\n",
    "                output_real = netD(real, training=True)\n",
    "                errD_real = -tf.reduce_mean(output_real)\n",
    "                \n",
    "                #generate a result with fake image and prev\n",
    "                fake = netG(tf.stop_gradient(noise), training=False)\n",
    "                width = int((prev.shape[1]-fake.shape[1])/2)\n",
    "                fake = fake + prev[:,width:(prev.shape[1]-width),width:(prev.shape[2]-width),:]\n",
    "                \n",
    "                #as the discriminator, the error for fake should be a\n",
    "                output_fake = netD(tf.stop_gradient(fake), training=True)\n",
    "                errD_fake = tf.reduce_mean(output_fake)\n",
    "                \n",
    "                #calculate the gradient_penalty\n",
    "                gradient_penalty = calc_gradient_penalty(netD, real, fake, opt.lambda_grad)\n",
    "                errD = errD_real + errD_fake + gradient_penalty\n",
    "            \n",
    "            gradients_of_discriminator = disc_tape.gradient(errD, netD.trainable_variables)\n",
    "            optimizerD.apply_gradients(zip(gradients_of_discriminator, netD.trainable_variables))\n",
    "        \n",
    "        # Update G network in Wasserstein GANs: maximize D(G(z))\n",
    "        \n",
    "        with tf.GradientTape(watch_accessed_variables=False, persistent=True) as extra_tape:\n",
    "            extra_tape.watch(netG.trainable_variables)\n",
    "            fake = netG(tf.stop_gradient(noise),training=True)\n",
    "            width = int((prev.shape[1]-fake.shape[1])/2)\n",
    "            fake = fake + prev[:,width:(prev.shape[1]-width),width:(prev.shape[2]-width),:]\n",
    "            D_output = netD(fake, training=False)\n",
    "            errG = -tf.reduce_mean(D_output)\n",
    "        \n",
    "        for j in range(3):\n",
    "\n",
    "            with tf.GradientTape(watch_accessed_variables=False,) as gen_tape:\n",
    "                gen_tape.watch(netG.trainable_variables)\n",
    "                #fake = netG(tf.stop_gradient(noise),training=True)\n",
    "                #width = int((prev.shape[1]-fake.shape[1])/2)\n",
    "                #fake = fake + prev[:,width:(prev.shape[1]-width),width:(prev.shape[2]-width),:]\n",
    "                \n",
    "                #for fake example we should maximize the absolute calue of D(fake)\n",
    "                #D_output = netD(fake, training=False)\n",
    "                #errG = -tf.reduce_mean(D_output)\n",
    "                \n",
    "                #scale the noise, accumulate the weighted noise with the noise get through G\n",
    "                Z_opt = opt.noise_amp*z_opt+z_prev\n",
    "\n",
    "                #generate the result from Z_opt (accumulated z_opt) and z_prev(last z)\n",
    "                result = netG(tf.stop_gradient(Z_opt), training=True)\n",
    "                width = int((z_prev.shape[1]-result.shape[1])/2)\n",
    "                result = result + z_prev[:,width:(z_prev.shape[1]-width),width:(z_prev.shape[2]-width),:]\n",
    "                #rec loss is the weighted MSE of real image and generated image\n",
    "                loss = tf.keras.losses.MeanSquaredError()\n",
    "                rec_loss = alpha*loss(result,real)\n",
    "\n",
    "                #total_G = errG + rec_loss\n",
    "                \n",
    "            #update\n",
    "            extra_gradient = extra_tape.gradient(errG, netG.trainable_variables)\n",
    "            #optimizerG.apply_gradients(zip(extra_gradient, netG.trainable_variables))\n",
    "            gradient_of_generator = gen_tape.gradient(rec_loss, netG.trainable_variables)\n",
    "            optimizerG.apply_gradients(zip(gradient_of_generator+extra_gradient, netG.trainable_variables))\n",
    "        \n",
    "        del extra_tape\n",
    "        if epoch % 250 == 0 or epoch == (opt.niter-1): \n",
    "        #if epoch == (opt.niter-1): #only saved once (for small graph)\n",
    "            plt.imsave(f'{opt.outf}/fake_sample.png', convert_image_np(tf.stop_gradient(fake)))\n",
    "            plt.imsave(f'{opt.outf}/G(z_opt).png', \n",
    "                       convert_image_np(tf.stop_gradient(G_process(netG, tf.stop_gradient(Z_opt), z_prev))), vmin=0, vmax=1)\n",
    "\n",
    "    save_networks(netG,netD,z_opt,opt)\n",
    "    return z_opt,in_s,netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOOyZD0IbKjb"
   },
   "outputs": [],
   "source": [
    "def train(real,opt,Gs,Zs,NoiseAmp):\n",
    "    in_s = 0\n",
    "    nfc_prev = 0\n",
    "    #resize the input real and generate the pyramid\n",
    "    real = imresize_scale_tensor(real,opt.scale1)\n",
    "    #print(real.shape)\n",
    "    reals = create_reals_pyramid(real,opt)\n",
    "    #parent directory\n",
    "    opt.out_ = generate_dir2save(opt)\n",
    "    \n",
    "    #in each scale\n",
    "    for scale_num in tqdm(range(opt.stop_scale+1), desc = opt.input_name, leave = True):\n",
    "        opt.nfc = min(opt.nfc_init * pow(2, math.floor(scale_num / 4)), 128)\n",
    "        opt.min_nfc = min(opt.min_nfc_init * pow(2, math.floor(scale_num / 4)), 128)\n",
    "        \n",
    "        #directory for each scale\n",
    "        opt.outf = f'{opt.out_}/{scale_num}'\n",
    "        try:\n",
    "            os.makedirs(opt.outf)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        #in the scale specific directory save the real_scale.png\n",
    "        plt.imsave(f'{opt.outf}/real_scale.png', convert_image_np(reals[scale_num]), vmin=0, vmax=1)\n",
    "        #train this scale and return the generator\n",
    "        z_curr,in_s,G_curr = train_single_scale(reals,Gs,Zs,in_s,NoiseAmp,opt, nfc_prev, scale_num)\n",
    "        \n",
    "        G_curr = freeze(G_curr)\n",
    "        #save them to each list\n",
    "        Gs.append(G_curr)#model\n",
    "        #print(z_curr.shape)\n",
    "        Zs.append(z_curr)#tensor\n",
    "        NoiseAmp.append(opt.noise_amp)#tensor\n",
    "        \n",
    "        #for future use, save all variables and models to the parent directory of the model\n",
    "        #which is easier to load in one time\n",
    "        save_vars(G_curr, Zs[-1], NoiseAmp[-1], reals[scale_num], opt, scale_num)\n",
    "        nfc_prev = opt.nfc\n",
    "        del G_curr\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling(im,sx,sy):\n",
    "    #bilinearlly upscale the data\n",
    "    new = tf.image.resize(im, size = [round(sx),round(sy)])\n",
    "    return new\n",
    "\n",
    "def generate_in2coarsest(reals,scale_v,scale_h,opt):\n",
    "    #pick the coarest scale image\n",
    "    real = reals[opt.gen_start_scale]\n",
    "    #upsample it back bilinearlly\n",
    "    real_down = upsampling(real, scale_v * real.shape[2], scale_h * real.shape[3])\n",
    "    #for fresh start\n",
    "    if opt.gen_start_scale == 0:\n",
    "        #generate from 0\n",
    "        in_s = tf.fill(real_down.shape, 0.)\n",
    "    else:\n",
    "        #otherwise start from real_down\n",
    "        in_s = upsampling(real_down, real_down.shape[2], real_down.shape[3])\n",
    "    return in_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinGAN_generate(Gs,Zs,reals,NoiseAmp,opt,in_s = None,gen_start_scale=0,num_samples=50,output_image = False):\n",
    "    # make in_s a 0 tensor with reals[0] shape\n",
    "    if in_s is None:\n",
    "        in_s = tf.fill(reals[0].shape, 0.)\n",
    "    #generate a pad width ((ker_size-1)*num_layer)/2\n",
    "    pad = int(((opt.ker_size-1)*opt.num_layer)/2)\n",
    "    output_list = []\n",
    "    images_cur = []\n",
    "    #print(in_s.shape, pad, reals[0].shape)\n",
    "    \n",
    "    #for each scale\n",
    "    for G,Z_opt,noise_amp,n in zip(Gs,Zs,NoiseAmp,range(len(Gs))):\n",
    "        #the shape inside padding\n",
    "        real_x = Z_opt.shape[1]-pad*2\n",
    "        real_y = Z_opt.shape[2]-pad*2\n",
    "\n",
    "        #get all the previous image\n",
    "        images_prev,images_cur = images_cur,[]\n",
    "        \n",
    "        #for the number of samples\n",
    "        for i in range(0,num_samples):\n",
    "            if n == 0:\n",
    "                #generate a single channel noise,broadcast to 3 channel, then padding it with 0\n",
    "                z_curr = generate_noise(size = [real_x,real_y,1])\n",
    "                z_curr = tf.broadcast_to(z_curr,[1,z_curr.shape[1],z_curr.shape[2],3])\n",
    "                z_curr = zero_pad(z_curr,pad)\n",
    "            else:\n",
    "                #generate noise with defined shape,padding\n",
    "                z_curr = generate_noise(size = [real_x, real_y, opt.nc_z])\n",
    "                z_curr = zero_pad(z_curr,pad)\n",
    "                \n",
    "            #if it's the first scale\n",
    "            if images_prev == []:\n",
    "                #use in_s as the first one, get a all zero one\n",
    "                I_prev = zero_pad(in_s, pad)\n",
    "            else:\n",
    "                #get the last image, resize it by 1/scale_factor\n",
    "                #cut into image shape(not quite necessary but in case there's rounding issue)\n",
    "                I_prev = images_prev[i]\n",
    "                I_prev = imresize_scale_tensor(I_prev,1/opt.scale_factor)\n",
    "                I_prev = I_prev[:, 0:reals[n].shape[1], 0:reals[n].shape[2],:]\n",
    "                #padding,cut into noise shape (similarly, not quite necessary)\n",
    "                I_prev = zero_pad(I_prev,pad)\n",
    "                I_prev = I_prev[:,0:z_curr.shape[1],0:z_curr.shape[2],:]\n",
    "            \n",
    "            #for human face generating\n",
    "            if n < gen_start_scale:\n",
    "                z_curr = Z_opt\n",
    "                \n",
    "            # amplify the z by the param, add the previous graph\n",
    "            z_in = noise_amp*(z_curr)+I_prev\n",
    "            I_curr = G_process(G,z_in,I_prev)\n",
    "            #for the last scale\n",
    "            if n == len(reals)-1:\n",
    "                output_graph = convert_image_np(tf.stop_gradient(I_curr))\n",
    "                #output_graph = convert_image_np(I_prev[:,width:(I_prev.shape[1]-width),width:(I_prev.shape[2]-width),:])\n",
    "                output_list.append(output_graph)\n",
    "                if (output_image):\n",
    "                    #generate the directory\n",
    "                    directory = os.path.join(generate_dir2save(opt), \"image_generated\")#modified\n",
    "                    try:\n",
    "                        os.makedirs(directory)\n",
    "                    except OSError:\n",
    "                        pass\n",
    "                    #save the new generated image\n",
    "                    plt.imsave(f'{directory}/{i}.png', output_graph, vmin=0,vmax=1)\n",
    "            # have the generated image into the list\n",
    "            images_cur.append(I_curr)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4gXnYnobKjd"
   },
   "source": [
    "# Copy from MNIST script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NB8JaTrBbKjd"
   },
   "outputs": [],
   "source": [
    "#This file is the final version of MNIST data processing\n",
    "#Note it's used for the modified version of SinGAN under directory SinGAN/.. not the official version --Alex\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def train_model(input_name, layer_number = 6, random_seed = 1, epochs = 2000, Scale_plus1 = True, additional_scale = False):\n",
    "    #configure the option\n",
    "    INPUT_DIR = os.path.join(os.getcwd(), \"input\")\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), \"output\")\n",
    "    RANDOM_SEED = random_seed\n",
    "    CHANNEL = 3 #Channel = 1 for SinGan still expect 3 channel, so use this\n",
    "    INPUT_NAME = input_name\n",
    "    LAYER_NUMBER = layer_number #5 layer * 3 scale is not quite okay for number 9\n",
    "    EPOCHS = epochs\n",
    "    SCALE_PLUS1 = Scale_plus1\n",
    "    ADDITIONAL_SCALE = additional_scale\n",
    "    GENERATION_START_SCALE = 0 # for generating 50 examples in the model document\n",
    "    parser_train = get_arguments()\n",
    "    parser_train.add_argument('--input_dir')\n",
    "    parser_train.add_argument('--input_name')\n",
    "    parser_train.add_argument('--mode')\n",
    "    \n",
    "    #newly added, have change in functions.adjust_scales2image --yihao\n",
    "    parser_train.add_argument('--scale_plus1', type=int, default = 1)\n",
    "    parser_train.add_argument('--additional_scale', type=int, default = 0)\n",
    "    parser_train.add_argument(\"--gen_start_scale\", type=int)\n",
    "    \n",
    "    opt_train = parser_train.parse_args([\"--input_dir\", INPUT_DIR, \n",
    "                                         \"--input_name\", INPUT_NAME, \n",
    "                                         \"--mode\", \"train\",\n",
    "                                         \"--manualSeed\", str(RANDOM_SEED),\n",
    "                                         \"--out\", OUTPUT_DIR,\n",
    "                                         \"--gen_start_scale\", str(GENERATION_START_SCALE),\n",
    "                                         \"--num_layer\", str(LAYER_NUMBER),\n",
    "                                         \"--nc_z\", str(CHANNEL),\n",
    "                                         \"--nc_im\", str(CHANNEL),\n",
    "                                         \"--niter\", str(EPOCHS),\n",
    "                                         \"--scale_plus1\", str(int(SCALE_PLUS1)),\n",
    "                                         \"--additional_scale\", str(int(ADDITIONAL_SCALE))\n",
    "                                        ])\n",
    "    opt_train = post_config(opt_train)\n",
    "    \n",
    "    # follows the SinGan operation process, slightly simplified\n",
    "    Gs = []\n",
    "    Zs = []\n",
    "    NoiseAmp = []\n",
    "    \n",
    "    #save path(note this function is modified)\n",
    "    dir2save = generate_dir2save(opt_train)\n",
    "    #if there's existed direction, stop it\n",
    "    if (os.path.exists(dir2save)):\n",
    "        print(f'layer={opt_train.num_layer}, iteration={opt_train.niter}, scale_factor={opt_train.scale_factor_init}, alpha={opt_train.alpha} model for {opt_train.input_name} already exist')\n",
    "    #else run the training\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(dir2save)\n",
    "        except OSError:\n",
    "            pass\n",
    "        #read the image\n",
    "        real = read_image_tensor(opt_train)\n",
    "        #decide scales\n",
    "        adjust_scales2image(real, opt_train)\n",
    "        #time the training\n",
    "        start = time.time()\n",
    "        #train\n",
    "        train(real, opt_train, Gs, Zs, NoiseAmp)\n",
    "        #stop timing\n",
    "        end = time.time()\n",
    "        print(f\"input = {opt_train.input_name} finished, traing time {end - start}s, for n_iter = {opt_train.niter}, layer_number = {opt_train.num_layer}\")\n",
    "        # generate the example 50 graphs\n",
    "        #SinGAN_generate(Gs, Zs, reals, NoiseAmp, \n",
    "        #                opt_train, \n",
    "        #                gen_start_scale=opt_train.gen_start_scale, \n",
    "        #                output_image = True)\n",
    "    \n",
    "def generate_data(class_label = 4, epochs = 2000, generate_size = 50, random_seed = 1, sample_size = 5, start_scale = 0, output_image = False):\n",
    "    '''return generated image in np.array form'''\n",
    "    #Configures\n",
    "    INPUT_DIR = os.path.join(os.getcwd(), \"input\")\n",
    "    RANDOM_SEED = random_seed\n",
    "    GENERATION_START_SCALE = start_scale #for human face topic\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), \"output\")\n",
    "    CHANNEL = 3 \n",
    "    INPUT_NAME = [f\"MNIST_{class_label}_input_{i}.png\" for i in range(sample_size)]\n",
    "    LAYER_NUMBER = 6\n",
    "    SCALE_PLUS1 = True\n",
    "    ADDITIONAL_SCALE = 0\n",
    "    EPOCHS = epochs\n",
    "    # the list takes all the result\n",
    "    output = []\n",
    "    parser_generate = get_arguments()\n",
    "    parser_generate.add_argument('--input_dir')\n",
    "    parser_generate.add_argument('--input_name')\n",
    "    parser_generate.add_argument('--mode')\n",
    "    parser_generate.add_argument('--gen_start_scale', type=int)\n",
    "    parser_generate.add_argument('--scale_plus1', type=int)\n",
    "    parser_generate.add_argument('--additional_scale', type=int)\n",
    "    # for all the trained samples\n",
    "    for i in tqdm(range(sample_size), desc = \"loading\", leave = False):\n",
    "        #run standard procedure, create a namespace\n",
    "        opt_generate = parser_generate.parse_args([\"--input_dir\", INPUT_DIR, \n",
    "                                                   \"--input_name\", INPUT_NAME[i], \n",
    "                                                   \"--mode\", \"random_samples\",\n",
    "                                                   \"--manualSeed\", str(RANDOM_SEED),\n",
    "                                                   \"--gen_start_scale\", str(GENERATION_START_SCALE),\n",
    "                                                   \"--out\", OUTPUT_DIR,\n",
    "                                                   \"--num_layer\", str(LAYER_NUMBER),\n",
    "                                                   \"--nc_z\", str(CHANNEL),\n",
    "                                                   \"--nc_im\", str(CHANNEL),\n",
    "                                                   \"--niter\", str(EPOCHS),\n",
    "                                                   \"--scale_plus1\", str(int(SCALE_PLUS1)),\n",
    "                                                   \"--additional_scale\", str(int(ADDITIONAL_SCALE))\n",
    "                                                  ])\n",
    "        opt_generate = post_config(opt_generate)\n",
    "        #print(opt_generate.input_name)\n",
    "        #save the path(note this function is modified)\n",
    "        dir2save = generate_dir2save(opt_generate)\n",
    "        #print(dir2save)\n",
    "        #read the file\n",
    "        real = read_image_tensor(opt_generate)\n",
    "        #adjust scales, write into opt_generate\n",
    "        adjust_scales2image(real, opt_generate)\n",
    "        #load the model\n",
    "        Gs, Zs, reals, NoiseAmp = load_trained_pyramid(opt_generate)\n",
    "        #in_s = generate_in2coarsest(reals,1,1,opt_generate)\n",
    "        #generate the output(the function is modified to generate list_output)\n",
    "        list_output = SinGAN_generate(Gs, Zs,  reals, NoiseAmp, opt_generate, output_image = output_image, gen_start_scale=0,\n",
    "                                     num_samples = int(np.ceil(generate_size/(sample_size+1))))\n",
    "        output += list_output\n",
    "    #change list into np.array\n",
    "    output = np.array(output)\n",
    "    #shuffle it by the first axis\n",
    "    np.random.shuffle(output)\n",
    "    return output[:generate_size]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Reproduce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
